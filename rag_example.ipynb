{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import load_eidc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = load_eidc_data.load_title_description_lineage('data/catalogue_metadata.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.document_stores import InMemoryDocumentStore\n",
    "\n",
    "doc_store = InMemoryDocumentStore(use_gpu=False, use_bm25=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating BM25 representation...: 100%|██████████| 1867/1867 [00:00<00:00, 3876.07 docs/s]\n"
     ]
    }
   ],
   "source": [
    "from haystack.schema import Document\n",
    "\n",
    "docs = [Document(content=d, id=i) for i, d in enumerate(texts)]\n",
    "doc_store.write_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mpc/github/discoverability/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from haystack.nodes import PromptNode, PromptTemplate, BM25Retriever, SentenceTransformersRanker\n",
    "\n",
    "retriever = BM25Retriever(document_store=doc_store, top_k=5)\n",
    "reranker = SentenceTransformersRanker(model_name_or_path='cross-encoder/ms-marco-MiniLM-L-12-v2', top_k=1)\n",
    "\n",
    "lfqa_prompt = PromptTemplate(prompt='Answer the question using the provided context. If you are not sure about the answer, answer with \"I do not know\". Your answer should be in your own words and be no longer than 100 words. \\\\n\\\\n Context: {join(documents)} \\\\n\\\\n Question: {query} \\\\n\\\\n Answer:\\\",\\n', output_parser={'type': 'AnswerParser'})\n",
    "prompt = PromptNode(model_name_or_path='MBZUAI/LaMini-Flan-T5-783M', default_prompt_template=lfqa_prompt,\n",
    "                    model_kwargs={'model_max_length': 2048, 'torch_dtype': torch.bfloat16})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "p = Pipeline()\n",
    "p.add_node(component=retriever, name=\"Retriever\", inputs=[\"Query\"])\n",
    "p.add_node(component=reranker, name=\"Reranker\", inputs=[\"Retriever\"])\n",
    "p.add_node(component=prompt, name=\"prompt_node\", inputs=[\"Reranker\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The data on surface height and reflectance were collected by the NERC Airborne Research and Survey Facility (ARSF).'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = p.run('Who collected the land cover map data?')\n",
    "a['answers'][0].answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Climate change and habitat destruction can affect butterfly populations.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = p.run('What can affect butterfly populations?')\n",
    "a['answers'][0].answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The provided context does not provide information about the age of the oldest man in the world.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = p.run('How old is the oldest man in the world?')\n",
    "a['answers'][0].answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The provided context does not mention which site has the wettest soil in the UK.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = p.run('Where is the wettest soil in the UK?')\n",
    "a['answers'][0].answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
